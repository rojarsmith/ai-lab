# 鏈式法則(Chain Rule)

用最白話的方式一步一步幫你理解：

------

# 🔗 什麼是 Chain Rule（鏈式法則）？

## ✅ 一句話解釋：

> **當你要對一個「包在另一個裡面」的函數做微分時，就要用 chain rule！**

它告訴你：

> **「先對外面微分，再對裡面微分，然後乘起來」**。

------

## 🎯 白話比喻：像剝洋蔥一樣微分！

想像你有一顆洋蔥（套了很多層函數）：

```python
z = f(g(x))
```

要對 `z` 相對於 `x` 求導數時，你不能一次就跳到底，而是要：

1. **先看外層**（f）
2. **再看內層**（g）
3. **然後把兩層的導數乘起來**

------

## 🧮 Chain Rule 數學公式（兩層）：

```math
\frac{dz}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}
```

意思是：

> **先微分外層 f（針對 g），再乘上內層 g 對 x 的導數**

------

## 🧪 舉個具體例子

假設：

```math
z = f(g(x)) = (3x + 2)^2
```

你可以設：

- g(x) = 3x + 2
- f(g) = g²

### 我們就可以用鏈式法則：

```math
dz/dx = df/dg · dg/dx = 2g(x) · 3 = 2(3x+2) · 3
```

✔️ 這樣比硬展開平方還快！

------

## 🤖 在深度學習裡有多重要？

**超級重要**。

神經網路每一層的輸出其實都是：

```text
z = fₙ(fₙ₋₁(fₙ₋₂(...f₁(x)...)))
```

所以在做 **反向傳播（backpropagation）** 時，就是從最後一層往回一層一層用 chain rule 算出每層參數的「梯度」。

------

## 📌 為什麼叫「鏈」式法則？

因為你會把一堆函數像「鏈條一樣」串在一起：

```text
L(w) = loss(output(w))
```

求導時就像順著一條鍊子，一個一個微分再乘起來。

------

## ✅ 小結表格

| 元素            | 白話解釋                             |
| --------------- | ------------------------------------ |
| f(g(x))         | 一層包一層的函數                     |
| Chain Rule 用途 | 微分嵌套函數                         |
| 微分方式        | 外層導數 × 內層導數                  |
| 深度學習應用    | 神經網路每層參數反向傳播用這套       |
| 比喻            | 洋蔥（一層一層剝）或鏈條（逐層處理） |

------

## a = sigmoid(z) = 1 / (1 + e^(-z))如何對其r進行z的偏導數

你這題其實就是在問：

```math
[a = \sigma(z) = \frac{1}{1 + e^{-z}}]
```

對 **z 做偏導數**（其實這裡是普通導數，因為只有一個變數）。

------

## 第一步：寫出原式

```math
[a = (1 + e^{-z})^{-1}]
```

------

## 第二步：使用鏈式法則 (Chain Rule)

對 z 微分：

```math
[\frac{da}{dz} = -1 \cdot (1 + e^{-z})^{-2} \cdot \frac{d}{dz}(1 + e^{-z})]
```

而：

```math
[\frac{d}{dz}(1 + e^{-z}) = -e^{-z}]
```

指數函數e的z偏微分-z不用減1。

------

## 第三步：整理

```math
[\frac{da}{dz} = -(1 + e^{-z})^{-2} \cdot (-e^{-z})] = [\frac{e^{-z}}{(1 + e^{-z})^2}]
```

------

## 第四步：用 sigmoid 本身表示（最漂亮的形式）

因為：

```math
[a = \frac{1}{1 + e^{-z}}]
```

且

```math
[1 - a = \frac{e^{-z}}{1 + e^{-z}}]
```

所以：

```math
[\frac{da}{dz} = a(1-a)]
```

------

# ✅ 最終答案

```math
[\boxed{\frac{d}{dz} \sigma(z) = \sigma(z)(1-\sigma(z))}]
```

或寫成：

```math
[\boxed{\frac{da}{dz} = a(1-a)}]
```

------

# 🔥 重要性

這個結果是：

- 神經網路
- Logistic Regression
- Backpropagation
- Binary Cross Entropy

的核心公式。

因為它讓梯度計算非常乾淨。

------

