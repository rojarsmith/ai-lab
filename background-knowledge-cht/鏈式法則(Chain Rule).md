# 鏈式法則(Chain Rule)

用最白話的方式一步一步幫你理解：

------

# 🔗 什麼是 Chain Rule（鏈式法則）？

## ✅ 一句話解釋：

> **當你要對一個「包在另一個裡面」的函數做微分時，就要用 chain rule！**

它告訴你：

> **「先對外面微分，再對裡面微分，然後乘起來」**。

------

## 🎯 白話比喻：像剝洋蔥一樣微分！

想像你有一顆洋蔥（套了很多層函數）：

```python
z = f(g(x))
```

要對 `z` 相對於 `x` 求導數時，你不能一次就跳到底，而是要：

1. **先看外層**（f）
2. **再看內層**（g）
3. **然後把兩層的導數乘起來**

------

## 🧮 Chain Rule 數學公式（兩層）：

```math
\frac{dz}{dx} = \frac{df}{dg} \cdot \frac{dg}{dx}
```

意思是：

> **先微分外層 f（針對 g），再乘上內層 g 對 x 的導數**

------

## 🧪 舉個具體例子

假設：

```math
z = f(g(x)) = (3x + 2)^2
```

你可以設：

- g(x) = 3x + 2
- f(g) = g²

### 我們就可以用鏈式法則：

```math
dz/dx = df/dg · dg/dx = 2g(x) · 3 = 2(3x+2) · 3
```

✔️ 這樣比硬展開平方還快！

------

## 🤖 在深度學習裡有多重要？

**超級重要**。

神經網路每一層的輸出其實都是：

```text
z = fₙ(fₙ₋₁(fₙ₋₂(...f₁(x)...)))
```

所以在做 **反向傳播（backpropagation）** 時，就是從最後一層往回一層一層用 chain rule 算出每層參數的「梯度」。

------

## 📌 為什麼叫「鏈」式法則？

因為你會把一堆函數像「鏈條一樣」串在一起：

```text
L(w) = loss(output(w))
```

求導時就像順著一條鍊子，一個一個微分再乘起來。

------

## ✅ 小結表格

| 元素            | 白話解釋                             |
| --------------- | ------------------------------------ |
| f(g(x))         | 一層包一層的函數                     |
| Chain Rule 用途 | 微分嵌套函數                         |
| 微分方式        | 外層導數 × 內層導數                  |
| 深度學習應用    | 神經網路每層參數反向傳播用這套       |
| 比喻            | 洋蔥（一層一層剝）或鏈條（逐層處理） |

------

