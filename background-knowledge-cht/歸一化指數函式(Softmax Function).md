# 歸一化指數函式(Softmax Function)

### 📘 白話解釋：什麼是「歸一化指數函式（Softmax Function）」？

------

#### ✅ 定義簡單說

Softmax 函式是一種**將一組數字轉換成機率分布**的數學工具，常用在分類問題的輸出層。例如：如果你有一個模型要分辨圖片是「貓、狗、鳥」其中之一，最後會得到三個數字（logits），Softmax 就是把這三個數字轉成「機率」的形式。

------

#### 🔢 怎麼轉？

Softmax 的轉換方式其實很簡單，就是**把每個數字先取指數（exp），再除以所有數字取指數的總和**，保證結果：

- 每個數值都在 0 到 1 之間
- 加起來剛好是 1（就像機率）

數學式是：

```math
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}
```

這裡：

- $z_i$ 是原始分數（logits）
- $e$ 是自然常數（大約 2.718）
- $\sum$ 是把所有結果加總

------

#### 📊 舉個例子（超簡單！）

假設你的模型最後輸出三個數字：

```
logits = [2.0, 1.0, 0.1]
```

我們套用 Softmax：

1. 取每個數字的指數：

   ```
   e^2.0 ≈ 7.39
   e^1.0 ≈ 2.72
   e^0.1 ≈ 1.11
   ```

2. 加總起來：

   ```
   7.39 + 2.72 + 1.11 ≈ 11.22
   ```

3. 各別除以總和：

   ```
   [7.39/11.22, 2.72/11.22, 1.11/11.22] ≈ [0.66, 0.24, 0.10]
   ```

➡️ 結果就代表：

- 第一個類別（例如貓）的機率是 66%
- 第二個類別（狗）是 24%
- 第三個類別（鳥）是 10%

------

#### 🧠 為什麼要用 Softmax？

- **機率感**：讓你知道模型「最有可能」選哪一類，並且能比較信心程度。
- **數學連貫**：它跟交叉熵損失（cross entropy loss）搭配得很好，是深度學習分類問題的常見組合。

------

#### 📍補充：Softmax vs Sigmoid？

- **Softmax**：多分類問題（多個類別，互斥）
- **Sigmoid**：二分類或多標籤問題（類別可以同時存在）

------

