# 超參數（Hyperparameters）

超參數（**Hyperparameters**）是在**機器學習或深度學習模型訓練之前，由人類事先設定的參數**，它們不會從資料中學出來，但會**影響模型如何學習**。

你可以把它理解成：

> 🔧 超參數 = 控制「學習過程」的設定值
> 🧠 參數（Parameters）= 模型真正從資料中學到的數值

------

# 一、參數 vs 超參數

| 類型                         | 誰決定？       | 例子                                  |
| ---------------------------- | -------------- | ------------------------------------- |
| **參數 (Parameters)**        | 模型自己學出來 | 神經網路的權重 (weights)、偏差 (bias) |
| **超參數 (Hyperparameters)** | 人類事先設定   | 學習率、批次大小、層數                |

------

# 二、常見超參數例子

### 1️⃣ 學習率（Learning Rate）

- 控制每次更新權重的「步伐大小」
- 太大 → 可能震盪不收斂
- 太小 → 訓練超慢

```
learning_rate = 0.001
```

------

### 2️⃣ 批次大小（Batch Size）

- 每次拿多少資料來算梯度
- 小 batch → 更新頻繁、較不穩定
- 大 batch → 穩定但吃記憶體

```
batch_size = 32
```

------

### 3️⃣ 訓練次數（Epoch）

- 整份資料跑幾輪

```
epochs = 100
```

------

### 4️⃣ 模型結構相關

- 神經網路層數
- 每層神經元數量
- Transformer 的 head 數量
- 隱藏層維度

------

### 5️⃣ 正則化相關

- dropout rate
- weight decay
- L2 regularization 係數

------

# 三、為什麼叫「超」參數？

因為它是「參數的參數」。

模型參數（權重）由學習率來控制如何更新，所以：

```
超參數 → 控制 → 參數 → 控制 → 預測結果
```

它在「參數之上」，所以叫 **Hyper-parameter**。

------

# 四、超參數怎麼找？

這是機器學習的核心技巧之一。

常見方法：

### 1️⃣ Grid Search

全部排列組合暴力測試

### 2️⃣ Random Search

隨機抽樣測試

### 3️⃣ Bayesian Optimization

用機率模型找最佳解

### 4️⃣ AutoML

------

# 五、直觀比喻

把模型想成一個學生：

| 元素        | 對應                             |
| ----------- | -------------------------------- |
| 權重 (參數) | 學生腦袋裡記住的知識             |
| 學習率      | 讀書節奏                         |
| batch size  | 每次讀幾頁                       |
| epoch       | 讀幾輪課本                       |
| dropout     | 是否刻意丟掉部分筆記防止過度依賴 |

------

# 六、如果用你現在在讀 Transformer 來說

例如：

```
d_model = 512
num_heads = 8
num_layers = 6
learning_rate = 1e-4
```

這些全部都是超參數。

而 Transformer 裡的：

```
W_q, W_k, W_v
```

才是模型真正學出來的參數。

------

# 七、一句話總結

> 超參數 = 人為設定、控制訓練過程的值
> 參數 = 模型從資料中學出來的值

------

