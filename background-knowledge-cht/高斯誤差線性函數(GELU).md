# 高斯誤差線性函數(GELU)

------

## 一句話版本

**GELU 是一種讓神經網路決定「這個數值要保留多少」的函數。**
它不像 ReLU 那樣直接砍掉負數，而是「機率式地保留一點點」。

------

## 先講背景：為什麼需要這種函數？

在神經網路裡，每一層都會經過一個「激活函數」（activation function）。

常見的像：

### 🔹 ReLU

```
x > 0  → 保留
x <= 0 → 變成 0
```

問題是：

- 只要是負數，直接變 0
- 有點「太粗暴」

------

## GELU 在做什麼？

GELU 全名：

> Gaussian Error Linear Unit

聽起來很可怕，但概念其實很直覺。

它的想法是：

> 一個數值該不該保留，取決於它「有多可能是好的訊號」

它會用「機率」的概念來決定保留多少。

------

## 白話理解

你可以這樣想：

- 很大的正數 → 幾乎完整保留
- 很小的負數 → 幾乎變 0
- 靠近 0 的數 → 保留一部分

不是「砍掉」，而是「柔順地縮小」。

------

## 跟 ReLU 比較

| x值  | ReLU | GELU       |
| ---- | ---- | ---------- |
| -3   | 0    | 接近0      |
| -0.5 | 0    | 小小負值   |
| 0.5  | 0.5  | 稍微小一點 |
| 3    | 3    | 幾乎3      |

👉 GELU 是「平滑版 ReLU」。

------

## 為什麼 Transformer 都用 GELU？

像：

- BERT
- GPT
- Transformer

幾乎都用 GELU。

原因：

1. 平滑（可微分）
2. 訓練更穩定
3. 在 NLP 任務效果比較好

因為語言資料是連續且細膩的訊號，
不像圖片那麼明確的邊界。

------

## 用生活比喻

ReLU 像：

> 「負的全部開除！」

GELU 像：

> 「負的可能只是小錯，留點觀察空間。」

------

## 數學公式（可跳過）

真正公式是：

```math
GELU(x)=x⋅Φ(x)
```

Φ(x) 是標準常態分布的累積機率。

意思是：

> 用「x 是正值的機率」來乘上 x

------

## 更直覺版本

實作時常用近似公式：

```math
0.5x(1+tanh(2/π​(x+0.044715x3)))
```

但你不用背。

只要記住：

> GELU = 用機率來決定保留多少輸出

------

很好，這題問得很關鍵 👍
你現在是在問「**GELU 為什麼會長這樣？它不是隨便湊出來的嗎？**」

我們一步一步用白話拆解。

------

# 一、核心想法：把神經元當成「會隨機出錯」的單位

GELU 的原始論文（2016）提出一個想法：

> 神經元的輸入其實帶有雜訊（noise）

也就是說：

假設某個輸入是 ( x )，
但實際上它可能是：

```math
x+ϵ
```

其中 ϵ ( epsilon ) 是隨機噪音，而且假設服從：

```math
ϵ∼N(0,1)
```

（標準常態分布）

------

# 二、關鍵概念：這個值「有多大機率是正的？」

如果輸入有隨機性，那我們可以問：

> 這個神經元最終是正的機率是多少？

也就是：

```math
P(x+ϵ>0)
```

整理一下：

```math
P(ϵ>−x)
```

因為 ε 是標準常態分布，

這個機率就等於：

```math
Φ(x)
```

其中：

```math
Φ(x)
```

就是「標準常態分布的 CDF（累積分布函數）」。

------

# 三、現在神來一筆

既然：

- x 是輸入大小
- Φ(x) 是它為正的機率

那我們乾脆讓輸出等於：

```math
x×P(它為正)
```

也就是：

```math
GELU(x)=xΦ(x)
```

這不是硬湊的。

它是：

> 「輸入大小」×「這個值是有效訊號的機率」

這其實是一種 **期望值（Expected value）** 的概念。

------

# 四、為什麼不是硬切（像 ReLU）？

ReLU 等於：

```math
x⋅1(x>0)
```

這其實就是：

> 用一個「硬的指示函數」

而 GELU 把這個硬開關：

```math
1(x>0)
```

換成：

```math
Φ(x)
```

變成平滑版本。

所以你可以這樣理解：

```math
GELU = Soft ReLU
```

------

# 五、那個可怕的 tanh 公式怎麼來的？

真正的 CDF：

```math
Φ(x)
```

沒有簡單閉式解。

為了計算快，論文給了一個近似式：

```math
Φ(x)≈0.5(1+tanh(2/π​(x+0.044715x3)))
```

這是數值近似（approximation），目的是：

- 計算快
- GPU 友好
- 誤差極小

------

# 六、數學本質總結

GELU 本質是：

```math
E[x⋅1(x+ϵ>0)]
=𝑥𝑃(𝑥+𝜖>0)
=𝑥Φ(𝑥)
```

它是：

> 把神經元看成隨機變數
> 用機率來加權輸出

這其實是「統計觀點」下的 activation function。

------

# 七、為什麼這在 Transformer 很有效？

因為：

- Transformer 的輸入分布常接近常態分布
- LayerNorm 讓資料接近 0 mean、unit variance
- 所以用 Gaussian-based 函數特別合理

這不是巧合。

是整個架構搭配的結果。

------

# 八、最精簡一句話

GELU 不是亂設計的。

它是：

> 把 ReLU 的硬開關
> 換成「值為正的機率」

背後來自常態分布 + 期望值概念。

------

